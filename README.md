# Visual-QA on Medical Images
Finetuned BLIP pretrained model on [PathVQA Dataset](https://huggingface.co/datasets/flaviagiammarino/path-vqa/viewer/default/train).

Datset looks like this:
![image](https://github.com/user-attachments/assets/b5afd54e-31f0-4105-ad4c-6311f0b6028c)

# References
[BLIP on huggingface](https://huggingface.co/docs/transformers/en/model_doc/blip)

[Paper link](https://arxiv.org/abs/2201.12086)

[Yannic's explanation](https://www.youtube.com/watch?v=X2k7n4FuI7c)
