# Visual-QA on Medical Images
Finetuned BLIP pretrained model on [PathVQA Dataset](https://huggingface.co/datasets/flaviagiammarino/path-vqa/viewer/default/train).

Dataset looks like this:
![image](https://github.com/user-attachments/assets/b5afd54e-31f0-4105-ad4c-6311f0b6028c)

During inference, the image is first encoded and then those vectors representing images is incorporated with the question text encoder and a new vector is generated which contains information about the question which was asked and the image. Now this new vector is passed into answer decoder model which basically generated the response. The image attached below explains this whole procedure:
![image](https://github.com/user-attachments/assets/99444611-f3ac-4307-88bb-f3622bdae9aa)

# References
[BLIP on huggingface](https://huggingface.co/docs/transformers/en/model_doc/blip)

[Paper link](https://arxiv.org/abs/2201.12086)

[Yannic's explanation](https://www.youtube.com/watch?v=X2k7n4FuI7c)

# Things to do:

Incorporate some metric like BLEU score to evaluate responses generated by the model.

Check the loss on validation dataset and also fix minor bug in that.

~Need to check that whether only LM loss is taken into account while finetuning for VQA specifically or we need to have all three losses combined (As per my reading of the paper we need to finetune it with LM loss only, but I need to verfiy this, in the current implemenation all three losses are taken into account while finetuning).~ Only LM loss is taken into account. Check this [link](https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_blip.py#L1349).

Simple gradio UI for direct usage.
